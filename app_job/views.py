from fake_useragent import UserAgent
from collections import Counter
from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker
from datetime import datetime
import time
import ast
import re
import requests as rq


from django.http import JsonResponse, HttpRequest
from django.shortcuts import render


from app_job.models import JobsData, JobCategoryTopKey


def home(request):
    return render(request,'app_job/home.html')

user_agent = UserAgent()
keywords = ["C#", "Python", "JavaScript"]


def fetch_104_jobs(keyword="Python", page=1, start_date=30):
    url = "https://www.104.com.tw/jobs/search/list"
    params = {
        "ro": "1", # å…¨è·
        "s9": "1",  # æ—¥ç­
        "keyword": keyword,
        "area": "6001016000",  # é«˜é›„
        "jobexp": "1",  # 1 å¹´ä»¥ä¸‹
        "page": str(page),
        "isnew": str(start_date),
        # "sctp": M # æœˆè–ª
        # "scmin": "35000", # èµ·è–ª
        "edu": "4", # å¤§å­¸ç•¢æ¥­
        "dep": "3006005000", # è³‡è¨Šç®¡ç†ç³»ç›¸é—œ
    }

    headers = {
        "User-Agent": user_agent.random,
        "Referer": "https://www.104.com.tw/jobs/search/"
    }

    res = rq.get(url, params=params, headers=headers, timeout=5)
    if res.ok:
        return res.json()["data"]["list"]
    else:
        print(f"è«‹æ±‚å¤±æ•—ï¼š{res.status_code}")
        return []


def extract_start_salary(text):
    # æ­£è¦è¡¨ç¤ºå¼ï¼šåŒ¹é…ã€Œæœˆè–ªã€é–‹é ­å¾Œé¢æ¥ä¸€ä¸²æ•¸å­—ï¼ˆæœ‰å¯èƒ½æœ‰é€—è™Ÿï¼‰
    match = re.search(r"æœˆè–ª\s*([\d,]+)", text)
    if match:
        # ç§»é™¤é€—è™Ÿï¼Œè½‰æˆæ•´æ•¸
        return int(match.group(1).replace(",", ""))
    return 28000

def jobs_to_db(jobs, category:str):

    for i, job in enumerate(jobs):

        cont = job['description']
        cont = re.sub(r'â—†\n*', '\n', cont)
        cont = re.sub(r'\xa0', '', cont)
        cont = re.sub(r"<[^>]+>", "", cont)
        cont = re.sub(r'[\[\]]', '', cont)

        link = job['link']['job']

        timestamp = datetime.now().date()
        update_date = datetime.strptime(job["appearDate"], "%Y%m%d").date()

        full_link = f"https:{link}"
        JobsData.objects.update_or_create(
            link = full_link,
            defaults={
                "id": f"{category}_{timestamp}_{i}",
                "category": category,
                "timestamp": timestamp,
                "updated_at": update_date,
                "title": job["jobName"],
                "content": cont,
                "company": job["custName"],
                "address": job["jobAddrNoDesc"] + job["jobAddress"],
                "link": full_link,
                "salary": extract_start_salary(job["salaryDesc"])
            }
        )



def update_job():
    
    today = datetime.now().date()
    print(today)

    if JobsData.objects.filter(timestamp=today).exists():
        print("â³ ä»Šå¤©å·²ç¶“æ›´æ–°éJobsDataï¼Œè·³éæ›´æ–°")
        return
    
    max_pages = 3

    for kw in keywords:
        print(f"ğŸ” æœå°‹é—œéµå­—ï¼š{kw}")
        cate_jobs = []

        for page in range(1, max_pages + 1):
            jobs = fetch_104_jobs(keyword=kw, page=page)
            print(f"  ç¬¬ {page} é ï¼šæŠ“åˆ° {len(jobs)} ç­†")
            cate_jobs.extend(jobs)
            time.sleep(5)  # â± é¿å…è¢«å°é–

        jobs_to_db(cate_jobs, kw)

    # å»ºç«‹æ¨¡å‹
    ws = CkipWordSegmenter(model="albert-tiny")
    pos = CkipPosTagger(model="albert-tiny")
    ner = CkipNerChunker(model="albert-tiny")

    ckiplab_word(ws, pos, ner)
    topkey_category_orm_save()

    print("âœ… 104 å·¥ä½œè³‡æ–™æ›´æ–°å®Œç•¢")


def word_frequency(wp_pair, allowPOS):
    filtered_words = []
    for word, pos in wp_pair:
        if (pos in allowPOS) & (len(word) >= 2):
            filtered_words.append(word)
    counter = Counter(filtered_words)
    return counter.most_common(200)

def ckiplab_word(ws:CkipWordSegmenter, pos:CkipPosTagger, ner:CkipNerChunker):

    # ç¯©é¸ä»Šå¤©çš„è³‡æ–™
    today = datetime.now().date()
    jobs = JobsData.objects.filter(timestamp=today)

    contents = [job.content for job in jobs]

    # CKIP è™•ç†
    tokens = ws(contents)
    tokens_pos = pos(tokens)
    entity_list = ner(contents)

    allowPOS = ['Na', 'Nb', 'Nc', 'VC']
    word_pos_pair = [list(zip(w, p)) for w, p in zip(tokens, tokens_pos)]
    tokens_v2 = [[w for w, p in wp if len(w) >= 2 and p in allowPOS] for wp in word_pos_pair]

    # é »ç‡åˆ†æï¼ˆä½ è‡ªå®šç¾©çš„å‡½æ•¸ï¼‰
    keyfreqs = [word_frequency(wp, allowPOS=allowPOS) for wp in word_pos_pair]

    # å¯«å›è³‡æ–™åº«
    for job, token, token_v2, wp, tf, ents in zip(jobs, tokens, tokens_v2, word_pos_pair, keyfreqs, entity_list):
        job.tokens = token
        job.token_pos = wp
        job.tokens_v2 = token_v2
        job.top_key_freq = tf
        job.entities = ents
        job.save()

    print("âœ… ä»Šå¤©çš„è·ç¼º NLP åˆ†æå®Œæˆï¼")


def topkey_category_orm_save():
    allowedPOS = ['Na', 'Nb', 'Nc']
    data = JobsData.objects.values_list('category', flat=True).distinct()
    top_group_words = get_top_words_orm(data, allowedPOS)

    # top_group_words æ˜¯ list of (category, top_keys)
    for category, top_keys in top_group_words:
        
        JobCategoryTopKey.objects.update_or_create(
            category=category,
            defaults={'top_keys': str(top_keys)}
        )
    print("âœ… å·²å°‡åˆ†é¡ top keys å­˜å…¥è³‡æ–™åº«")

def get_top_words_orm(keywords, allowedPOS):
    
    top_cate_words = {}
    counter_all = Counter()

    for category in keywords:
        # å–å‡ºè©²åˆ†é¡çš„æ‰€æœ‰ Jobï¼Œä¸” token_pos ä¸ç‚ºç©º
        qs = JobsData.objects.filter(category=category).exclude(token_pos__isnull=True)

        words_group = []

        # é€ç­†è³‡æ–™è™•ç†
        for job in qs.iterator():  # iteratoré¿å…è¼‰å…¥éå¤šè³‡æ–™é€²è¨˜æ†¶é«”
            
            token_pos_list = ast.literal_eval(job.token_pos)

            flat_token_pos = [
                (word, pos)
                for word, pos in token_pos_list
                if isinstance(word, str) and isinstance(pos, str)
            ]

            filtered_words = [
                word for word, pos in flat_token_pos
                if len(word.strip()) >= 2 and pos in allowedPOS
            ]
            words_group.extend(filtered_words)

        counter = Counter(words_group)
        counter_all.update(counter)

        topwords = counter.most_common(100)
        top_cate_words[category] = topwords

    # 'å…¨éƒ¨'åˆ†é¡è©é »
    top_cate_words['ALL'] = counter_all.most_common(100)
    print(top_cate_words)
    return top_cate_words.items()


update_job()


# POST: csrf_exempt should be used
# æŒ‡å®šé€™ä¸€æ”¯ç¨‹å¼å¿½ç•¥csrfé©—è­‰
from django.views.decorators.csrf import csrf_exempt
@csrf_exempt
def api_get_job(request:HttpRequest):
    cate = request.POST.get('news_category')
    #cate = request.GET['news_category'] # this command also works.
    topk = request.POST.get('topk')
    topk = int(topk)
    
    chart_data, wf_pairs = get_category_job_orm(cate, topk)
    response = {'chart_data': chart_data,
         'wf_pairs': wf_pairs,
         }
    return JsonResponse(response)


def get_category_job_orm(cate, topk=10):
    # ORM   
    queryset = JobCategoryTopKey.objects.filter(category=cate).values('top_keys')
    if queryset.exists():
        top_keys_str = queryset[0]['top_keys']
        wf_pairs = ast.literal_eval(top_keys_str)[0:topk]
    else:
        wf_pairs = []    
    
    words = [w for w, f in wf_pairs]
    freqs = [f for w, f in wf_pairs]
    chart_data = {
        "category": cate,
        "labels": words,
        "values": freqs}
    #print(chart_data)
    return chart_data, wf_pairs

print("104_jobs--é¡åˆ¥ç†±é–€é—œéµå­—è¼‰å…¥æˆåŠŸ!")

